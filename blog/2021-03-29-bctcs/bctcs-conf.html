<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2023-03-25 Sat 17:14 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>British Colloquium for Theoretical Computer Science (BCTCS) 2021</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Jay Morgan">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" type="text/css" href="/css/general.css"/>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
</head>
<body>
<div id="preamble" class="status">
<h1 class="title">British Colloquium for Theoretical Computer Science (BCTCS) 2021</h1>
          <p class="subtitle">29-03-2021</p>
</div>
<div id="content">
<h1 class="title">British Colloquium for Theoretical Computer Science (BCTCS) 2021</h1>
<div id="topbar">
<img id="profile-picture" src="/images/profile.jpg" alt="Jay Paul Morgan profile picture" width="200px"/>
<h3>Dr. Jay Paul Morgan</h3>
<a href="/">About</a>
<a href="/blog">Blog Posts</a>
<a href="/news">News</a>
<a href="https://pageperso.lis-lab.fr/jay.morgan/teaching.html">Teaching</a>
<p id="social-links">
<a href="https://scholar.google.com/citations?user=AO1az5YAAAAJ&hl=fr"><img src="/images/google-scholar.png" alt="Jay Paul Morgan google scholar publication" width="30px" height="30px"/></a>
<a href="https://emacs.ch/@jaymorgan"><img src="/images/mastodon.png" alt="Jay Paul Morgan Mastodon" width="30px" height="30px"/></a>
<a href="https://github.com/jaypmorgan"><img src="/images/github.png" alt="Jay Paul Morgan jaypmorgan github link" width="30px" height="30px"/></a>
<a href="https://orcid.org/my-orcid?orcid=0000-0003-3719-362X"><img src="/images/orcid.png" alt="Jay Paul Morgan orcid link" width="30px" height="30px"/></a>
<a href="https://www.researchgate.net/profile/Jay-Morgan?ev=hdr_xprf"><img src="/images/researchgate.png" alt="Jay Paul Morgan research gate social link" width="30px" height="30px"/></a>
</p>
</div>

<p>
<b><b>Presentation Abstract</b></b>
</p>

<p>
Machine Learning (ML) has had a remarkable impact on society. Everything from the
phones in our pockets, to the cars that we drive, are being increasingly outfitted
with this progressively sophisticated suite of algorithms. But while many of the most
basic and fundamental algorithms from ML can be formally verified and tested for
safety without much trouble, the same may not be said for Deep Learning (DL) – a
prominent forerunner in the state-of-the-art for ML research. These DL models, while
performing simple matrix-to-matrix operations at a micro-level, have evolved in scale
far past what is tractable for current formal verification methods – all in the
pursuit of improving accuracy and performance. This issue of tractability is
unsettling considering that the existence of adversarial examples is well known in
the ML community. These adversarial examples occur when very small changes to the
input space result in a large change in the output space and cause a
miss-classification made by the DL model. In the context of self-driving vehicles,
small defects and visual artifacts in the sensor input of the DL model, could lead
the vehicle to wrongly conclude a stop sign indicates to continue driving where it
should have stopped. While the manufacturers will need to put safe-guards in place to
prevent this from happening, we should formally prove the (non)-existence of these
adversarial examples in the DL model itself. In this presentation, I present the
foundational knowledge for understanding adversarial examples, how we can use the
input space to dictate the search space for the existence of these examples, and
demonstrate their presence with the use of SAT-solving. This work, as a free and
open-source project, provides a framework for ML practitioners to verify their own
architectures.
</p>

<p>
<b><b>Presentation Slides</b></b>
</p>

<p>
<a href="./bctcs_presentation/presentation.html">Slides</a>
</p>
</div>
<div id="postamble" class="status">
<p class="date">Date: 29-03-2021</p>
<p class="author">Author: Jay Morgan</p>
<p class="date">Created: 2023-03-25 Sat 17:14</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
